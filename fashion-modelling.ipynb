{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import regex as re\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fashionEDA import fashionCleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping Data, Null Entry: 1\n",
      "Scrapping Data, Null Entry: 2\n",
      "Scrapping Data, Null Entry: 3\n",
      "Scrapping Data, Null Entry: 4\n",
      "Scrapping Data, Null Entry: 5\n",
      "Scrapping Data, Null Entry: 6\n",
      "Scrapping Data, Null Entry: 7\n",
      "Scrapping Data, Null Entry: 8\n",
      "Scrapping Data, Null Entry: 9\n",
      "Scrapping Data, Null Entry: 10\n",
      "Scrapping Data, Null Entry: 11\n",
      "Scrapping Data, Null Entry: 12\n",
      "Scrapping Data, Null Entry: 13\n",
      "Scrapping Data, Null Entry: 14\n",
      "Scrapping Data, Null Entry: 15\n",
      "Scrapping Data, Null Entry: 16\n",
      "Scrapping Data, Null Entry: 17\n",
      "Scrapping Data, Null Entry: 18\n",
      "Scrapping Data, Null Entry: 19\n",
      "Scrapping Data, Null Entry: 20\n",
      "Scrapping Data, Null Entry: 21\n",
      "Scrapping Data, Null Entry: 22\n",
      "Scrapping Data, Null Entry: 23\n",
      "Scrapping Data, Null Entry: 24\n",
      "Scrapping Data, Null Entry: 25\n",
      "Scrapping Data, Null Entry: 26\n",
      "Scrapping Data, Null Entry: 27\n",
      "Scrapping Data, Null Entry: 28\n",
      "Scrapping Data, Null Entry: 29\n",
      "Scrapping Data, Null Entry: 30\n",
      "Scrapping Data, Null Entry: 31\n",
      "Scrapping Data, Null Entry: 32\n",
      "Scrapping Data, Null Entry: 33\n",
      "Scrapping Data, Null Entry: 34\n",
      "Scrapping Data, Null Entry: 35\n",
      "Scrapping Data, Null Entry: 36\n",
      "Scrapping Data, Null Entry: 37\n",
      "Scrapping Data, Null Entry: 38\n",
      "Scrapping Data, Null Entry: 39\n",
      "Scrapping Data, Null Entry: 40\n",
      "Scrapping Data, Null Entry: 41\n",
      "Scrapping Data, Null Entry: 42\n",
      "Scrapping Data, Null Entry: 43\n",
      "Scrapping Data, Null Entry: 44\n",
      "Scrapping Data, Null Entry: 45\n",
      "Scrapping Data, Null Entry: 46\n",
      "Scrapping Data, Null Entry: 47\n",
      "Scrapping Data, Null Entry: 48\n",
      "Scrapping Data, Null Entry: 49\n",
      "Scrapping Data, Null Entry: 50\n",
      "Scrapping Data, Null Entry: 51\n",
      "Scrapping Data, Null Entry: 52\n",
      "Scrapping Data, Null Entry: 53\n",
      "Scrapping Data, Null Entry: 54\n",
      "Scrapping Data, Null Entry: 55\n",
      "Scrapping Data, Null Entry: 56\n",
      "Scrapping Data, Null Entry: 57\n",
      "Scrapping Data, Null Entry: 58\n",
      "Scrapping Data, Null Entry: 59\n",
      "Scrapping Data, Null Entry: 60\n",
      "Scrapping Data, Null Entry: 61\n",
      "Scrapping Data, Null Entry: 62\n",
      "Scrapping Data, Null Entry: 63\n",
      "Scrapping Data, Null Entry: 64\n",
      "Scrapping Data, Null Entry: 65\n",
      "Scrapping Data, Null Entry: 66\n",
      "Scrapping Data, Null Entry: 67\n",
      "Scrapping Data, Null Entry: 68\n",
      "Scrapping Data, Null Entry: 69\n",
      "Scrapping Data, Null Entry: 70\n",
      "Scrapping Data, Null Entry: 71\n",
      "Scrapping Data, Null Entry: 72\n",
      "Scrapping Data, Null Entry: 73\n",
      "Scrapping Data, Null Entry: 74\n",
      "Scrapping Data, Null Entry: 75\n",
      "Scrapping Data, Null Entry: 76\n",
      "Scrapping Data, Null Entry: 77\n",
      "Scrapping Data, Null Entry: 78\n",
      "Scrapping Data, Null Entry: 79\n",
      "Scrapping Data, Null Entry: 80\n",
      "Scrapping Data, Null Entry: 81\n",
      "Scrapping Data, Null Entry: 82\n"
     ]
    }
   ],
   "source": [
    "fc = fashionCleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit</th>\n",
       "      <th>user_id</th>\n",
       "      <th>bust size</th>\n",
       "      <th>item_id</th>\n",
       "      <th>weight</th>\n",
       "      <th>rating</th>\n",
       "      <th>rented for</th>\n",
       "      <th>review_text</th>\n",
       "      <th>body type</th>\n",
       "      <th>review_summary</th>\n",
       "      <th>category</th>\n",
       "      <th>height</th>\n",
       "      <th>size</th>\n",
       "      <th>age</th>\n",
       "      <th>review_date</th>\n",
       "      <th>bust</th>\n",
       "      <th>cup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fit</td>\n",
       "      <td>420272</td>\n",
       "      <td>34d</td>\n",
       "      <td>2260466</td>\n",
       "      <td>137</td>\n",
       "      <td>10</td>\n",
       "      <td>vacation</td>\n",
       "      <td>An adorable romper! Belt and zipper were a lit...</td>\n",
       "      <td>hourglass</td>\n",
       "      <td>So many compliments!</td>\n",
       "      <td>romper</td>\n",
       "      <td>68</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>2016-04-20</td>\n",
       "      <td>34</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fit</td>\n",
       "      <td>273551</td>\n",
       "      <td>34b</td>\n",
       "      <td>153475</td>\n",
       "      <td>132</td>\n",
       "      <td>10</td>\n",
       "      <td>other</td>\n",
       "      <td>I rented this dress for a photo shoot. The the...</td>\n",
       "      <td>straight &amp; narrow</td>\n",
       "      <td>I felt so glamourous!!!</td>\n",
       "      <td>gown</td>\n",
       "      <td>66</td>\n",
       "      <td>12</td>\n",
       "      <td>36</td>\n",
       "      <td>2013-06-18</td>\n",
       "      <td>34</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fit</td>\n",
       "      <td>909926</td>\n",
       "      <td>34c</td>\n",
       "      <td>126335</td>\n",
       "      <td>135</td>\n",
       "      <td>8</td>\n",
       "      <td>formal affair</td>\n",
       "      <td>I rented this for my company's black tie award...</td>\n",
       "      <td>pear</td>\n",
       "      <td>Dress arrived on time and in perfect condition.</td>\n",
       "      <td>dress</td>\n",
       "      <td>65</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>2014-02-12</td>\n",
       "      <td>34</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fit</td>\n",
       "      <td>151944</td>\n",
       "      <td>34b</td>\n",
       "      <td>616682</td>\n",
       "      <td>145</td>\n",
       "      <td>10</td>\n",
       "      <td>wedding</td>\n",
       "      <td>I have always been petite in my upper body and...</td>\n",
       "      <td>athletic</td>\n",
       "      <td>Was in love with this dress !!!</td>\n",
       "      <td>gown</td>\n",
       "      <td>69</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>2016-09-26</td>\n",
       "      <td>34</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fit</td>\n",
       "      <td>734848</td>\n",
       "      <td>32b</td>\n",
       "      <td>364092</td>\n",
       "      <td>138</td>\n",
       "      <td>8</td>\n",
       "      <td>date</td>\n",
       "      <td>Didn't actually wear it. It fit perfectly. The...</td>\n",
       "      <td>athletic</td>\n",
       "      <td>Traditional with a touch a sass</td>\n",
       "      <td>dress</td>\n",
       "      <td>68</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>2016-04-30</td>\n",
       "      <td>32</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit user_id bust size  item_id  weight  rating     rented for  \\\n",
       "0  fit  420272       34d  2260466     137      10       vacation   \n",
       "1  fit  273551       34b   153475     132      10          other   \n",
       "2  fit  909926       34c   126335     135       8  formal affair   \n",
       "3  fit  151944       34b   616682     145      10        wedding   \n",
       "4  fit  734848       32b   364092     138       8           date   \n",
       "\n",
       "                                         review_text          body type  \\\n",
       "0  An adorable romper! Belt and zipper were a lit...          hourglass   \n",
       "1  I rented this dress for a photo shoot. The the...  straight & narrow   \n",
       "2  I rented this for my company's black tie award...               pear   \n",
       "3  I have always been petite in my upper body and...           athletic   \n",
       "4  Didn't actually wear it. It fit perfectly. The...           athletic   \n",
       "\n",
       "                                     review_summary category  height  size  \\\n",
       "0                              So many compliments!   romper      68    14   \n",
       "1                           I felt so glamourous!!!     gown      66    12   \n",
       "2  Dress arrived on time and in perfect condition.     dress      65     8   \n",
       "3                   Was in love with this dress !!!     gown      69    12   \n",
       "4                   Traditional with a touch a sass    dress      68     8   \n",
       "\n",
       "   age review_date  bust cup  \n",
       "0   28  2016-04-20    34   d  \n",
       "1   36  2013-06-18    34   b  \n",
       "2   34  2014-02-12    34   c  \n",
       "3   27  2016-09-26    34   b  \n",
       "4   45  2016-04-30    32   b  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_df = fc.getData()\n",
    "fashion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brianhuang/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/brianhuang/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the NLTK English tokenizer and the stopwords of all languages\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = \" \".join(list(fashion_df['review_text'].values))\n",
    "summary = \" \".join(list(fashion_df['review_summary'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words.split()\n",
    "summary = summary.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(string.punctuation)\n",
    "\n",
    "clean_words = [] \n",
    "for i in range(len(words)):\n",
    "    clean_words.append(''.join([c for c in words[i].lower() if not c in punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added argument to this to include ngrams \n",
    "vectorizer = CountVectorizer(analyzer='word',ngram_range=(2, 2),\n",
    "                        max_features = 10000,\n",
    "                        tokenizer=word_tokenize,\n",
    "                        stop_words=stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True,\n",
    "                        analyzer='word',\n",
    "                        max_features=1500,\n",
    "                        tokenizer=word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logisitic and linear models for prediction \n",
    "mod = LinearRegression()\n",
    "mod2 = LogisticRegression()\n",
    "mod_cup = LinearRegression()\n",
    "mod2_cup = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_is = ['weight', 'size', 'height', 'age', 'rating']\n",
    "ohe = ['rented for', 'body type', 'category']\n",
    "text_vec = ['review_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cahnged this to vectorizer instead of tfidf\n",
    "text_transformer = make_column_transformer((vectorizer, 'review_summary'), remainder = 'passthrough')\n",
    "#so we can use vectorizer on the review_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ct = ColumnTransformer(\n",
    "    transformers = [(\"as_is\", FunctionTransformer(lambda x: x), as_is),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown = 'ignore'), ohe),\n",
    "    (\"vec\", text_transformer, text_vec)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use two models, linear and logisitic regression, both work alot better when we make the bust size ordinal rather than numerical\n",
    "# i also did the same for cup prediction as well\n",
    "pl = Pipeline(steps = [('transform', ct), ('Linregression', mod)])\n",
    "pl2 = Pipeline(steps = [('transform', ct), ('Logregression', mod2)])\n",
    "pl_cup = Pipeline(steps = [('transform', ct), ('Linregression', mod_cup)])\n",
    "pl2_cup = Pipeline(steps = [('transform', ct), ('Logregression', mod2_cup)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(fashion_df, test_size = 0.2, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bust_category(bust):\n",
    "    #encodes bust size into classes 1-10\n",
    "    if bust == 28:\n",
    "        return 0\n",
    "    elif bust == 30:\n",
    "        return 1\n",
    "    elif bust == 32:\n",
    "        return 2\n",
    "    elif bust == 34:\n",
    "        return 3\n",
    "    elif bust == 36:\n",
    "        return 4\n",
    "    elif bust == 38:\n",
    "        return 5\n",
    "    elif bust == 40:\n",
    "        return 6\n",
    "    elif bust == 42:\n",
    "        return 7\n",
    "    elif bust == 44:\n",
    "        return 8\n",
    "    elif bust == 46:\n",
    "        return 9\n",
    "    else:\n",
    "        return 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cup_category(cup):\n",
    "     #encodes cup size into classes 1-12\n",
    "    if cup == \"aa\":\n",
    "        return 0\n",
    "    elif cup == \"a\":\n",
    "        return 1\n",
    "    elif cup == \"b\":\n",
    "        return 2\n",
    "    elif cup == \"c\":\n",
    "        return 3\n",
    "    elif cup == \"d\":\n",
    "        return 4\n",
    "    elif cup == \"d+\":\n",
    "        return 5\n",
    "    elif cup == \"dd\":\n",
    "        return 6\n",
    "    elif cup == \"ddd/e\":\n",
    "        return 7\n",
    "    elif cup == \"f\":\n",
    "        return 8\n",
    "    elif cup == \"g\":\n",
    "        return 9\n",
    "    elif cup == \"h\":\n",
    "        return 10\n",
    "    elif cup == \"i\":\n",
    "        return 11\n",
    "    else:\n",
    "        return 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-37161a591f23>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"bust_cat\"] = train[\"bust\"].apply(bust_category)\n",
      "<ipython-input-24-37161a591f23>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"bust_cat\"] = test[\"bust\"].apply(bust_category)\n",
      "<ipython-input-24-37161a591f23>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"cup_cat\"] = train[\"cup\"].apply(cup_category)\n",
      "<ipython-input-24-37161a591f23>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"cup_cat\"] = test[\"cup\"].apply(cup_category)\n"
     ]
    }
   ],
   "source": [
    "train[\"bust_cat\"] = train[\"bust\"].apply(bust_category)\n",
    "test[\"bust_cat\"] = test[\"bust\"].apply(bust_category)\n",
    "train[\"cup_cat\"] = train[\"cup\"].apply(cup_category)\n",
    "test[\"cup_cat\"] = test[\"cup\"].apply(cup_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = train[['weight', 'size', 'height', 'age', 'rating', 'rented for', 'body type', 'category', 'review_summary']], train['bust_cat']\n",
    "test_X, test_Y = test[['weight', 'size', 'height', 'age', 'rating', 'rented for', 'body type', 'category', 'review_summary']], test['bust_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training/test for X/Y but for cup categories\n",
    "train_X, train_Y_cup = train[['weight', 'size', 'height', 'age', 'rating', 'rented for', 'body type', 'category', 'review_summary']], train['cup_cat']\n",
    "test_X, test_Y_cup = test[['weight', 'size', 'height', 'age', 'rating', 'rented for', 'body type', 'category', 'review_summary']], test['cup_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianhuang/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/brianhuang/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('transform',\n",
       "                 ColumnTransformer(transformers=[('as_is',\n",
       "                                                  FunctionTransformer(func=<function <lambda> at 0x7fa44d7301f0>),\n",
       "                                                  ['weight', 'size', 'height',\n",
       "                                                   'age', 'rating']),\n",
       "                                                 ('ohe',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['rented for', 'body type',\n",
       "                                                   'category']),\n",
       "                                                 ('vec',\n",
       "                                                  ColumnTransformer(remainder='passthrough',\n",
       "                                                                    transformers=[('countvectorizer',\n",
       "                                                                                   CountVector...\n",
       "                                                                                                               'my',\n",
       "                                                                                                               'myself',\n",
       "                                                                                                               'we',\n",
       "                                                                                                               'our',\n",
       "                                                                                                               'ours',\n",
       "                                                                                                               'ourselves',\n",
       "                                                                                                               'you',\n",
       "                                                                                                               \"you're\",\n",
       "                                                                                                               \"you've\",\n",
       "                                                                                                               \"you'll\",\n",
       "                                                                                                               \"you'd\",\n",
       "                                                                                                               'your',\n",
       "                                                                                                               'yours',\n",
       "                                                                                                               'yourself',\n",
       "                                                                                                               'yourselves',\n",
       "                                                                                                               'he',\n",
       "                                                                                                               'him',\n",
       "                                                                                                               'his',\n",
       "                                                                                                               'himself',\n",
       "                                                                                                               'she',\n",
       "                                                                                                               \"she's\",\n",
       "                                                                                                               'her',\n",
       "                                                                                                               'hers',\n",
       "                                                                                                               'herself',\n",
       "                                                                                                               'it',\n",
       "                                                                                                               \"it's\",\n",
       "                                                                                                               'its',\n",
       "                                                                                                               'itself', ...],\n",
       "                                                                                                   tokenizer=<function word_tokenize at 0x7fa4797b6820>),\n",
       "                                                                                   'review_summary')]),\n",
       "                                                  ['review_summary'])])),\n",
       "                ('Logregression', LogisticRegression())])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting linear and logistic models for predicting cup size\n",
    "# pl_cup.fit(train_X, train_Y_cup)\n",
    "pl2_cup.fit(train_X, train_Y_cup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl_cup.score(test_X,test_Y_cup) #linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35655975680568364"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl2_cup.score(test_X,test_Y_cup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianhuang/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('transform',\n",
       "                 ColumnTransformer(transformers=[('as_is',\n",
       "                                                  FunctionTransformer(func=<function <lambda> at 0x7fa44d7301f0>),\n",
       "                                                  ['weight', 'size', 'height',\n",
       "                                                   'age', 'rating']),\n",
       "                                                 ('ohe',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['rented for', 'body type',\n",
       "                                                   'category']),\n",
       "                                                 ('vec',\n",
       "                                                  ColumnTransformer(remainder='passthrough',\n",
       "                                                                    transformers=[('countvectorizer',\n",
       "                                                                                   CountVector...\n",
       "                                                                                                               'my',\n",
       "                                                                                                               'myself',\n",
       "                                                                                                               'we',\n",
       "                                                                                                               'our',\n",
       "                                                                                                               'ours',\n",
       "                                                                                                               'ourselves',\n",
       "                                                                                                               'you',\n",
       "                                                                                                               \"you're\",\n",
       "                                                                                                               \"you've\",\n",
       "                                                                                                               \"you'll\",\n",
       "                                                                                                               \"you'd\",\n",
       "                                                                                                               'your',\n",
       "                                                                                                               'yours',\n",
       "                                                                                                               'yourself',\n",
       "                                                                                                               'yourselves',\n",
       "                                                                                                               'he',\n",
       "                                                                                                               'him',\n",
       "                                                                                                               'his',\n",
       "                                                                                                               'himself',\n",
       "                                                                                                               'she',\n",
       "                                                                                                               \"she's\",\n",
       "                                                                                                               'her',\n",
       "                                                                                                               'hers',\n",
       "                                                                                                               'herself',\n",
       "                                                                                                               'it',\n",
       "                                                                                                               \"it's\",\n",
       "                                                                                                               'its',\n",
       "                                                                                                               'itself', ...],\n",
       "                                                                                                   tokenizer=<function word_tokenize at 0x7fa4797b6820>),\n",
       "                                                                                   'review_summary')]),\n",
       "                                                  ['review_summary'])])),\n",
       "                ('Linregression', LinearRegression())])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pl.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [round(pred) for pred in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.571574956450456"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 if p == a else 0 for p, a in zip(preds, test_Y)])/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5006660518495748"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(preds, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianhuang/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/Users/brianhuang/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('transform',\n",
       "                 ColumnTransformer(transformers=[('as_is',\n",
       "                                                  FunctionTransformer(func=<function <lambda> at 0x7fa44d7301f0>),\n",
       "                                                  ['weight', 'size', 'height',\n",
       "                                                   'age', 'rating']),\n",
       "                                                 ('ohe',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['rented for', 'body type',\n",
       "                                                   'category']),\n",
       "                                                 ('vec',\n",
       "                                                  ColumnTransformer(remainder='passthrough',\n",
       "                                                                    transformers=[('countvectorizer',\n",
       "                                                                                   CountVector...\n",
       "                                                                                                               'my',\n",
       "                                                                                                               'myself',\n",
       "                                                                                                               'we',\n",
       "                                                                                                               'our',\n",
       "                                                                                                               'ours',\n",
       "                                                                                                               'ourselves',\n",
       "                                                                                                               'you',\n",
       "                                                                                                               \"you're\",\n",
       "                                                                                                               \"you've\",\n",
       "                                                                                                               \"you'll\",\n",
       "                                                                                                               \"you'd\",\n",
       "                                                                                                               'your',\n",
       "                                                                                                               'yours',\n",
       "                                                                                                               'yourself',\n",
       "                                                                                                               'yourselves',\n",
       "                                                                                                               'he',\n",
       "                                                                                                               'him',\n",
       "                                                                                                               'his',\n",
       "                                                                                                               'himself',\n",
       "                                                                                                               'she',\n",
       "                                                                                                               \"she's\",\n",
       "                                                                                                               'her',\n",
       "                                                                                                               'hers',\n",
       "                                                                                                               'herself',\n",
       "                                                                                                               'it',\n",
       "                                                                                                               \"it's\",\n",
       "                                                                                                               'its',\n",
       "                                                                                                               'itself', ...],\n",
       "                                                                                                   tokenizer=<function word_tokenize at 0x7fa4797b6820>),\n",
       "                                                                                   'review_summary')]),\n",
       "                                                  ['review_summary'])])),\n",
       "                ('Logregression', LogisticRegression())])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl2.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5903610342589746"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl2.score(test_X, test_Y) #logistic regression on bust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_cup = RandomForestClassifier(max_depth = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_cup3 = Pipeline(steps = [('transform', ct), ('randforest', forest_cup)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianhuang/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('transform',\n",
       "                 ColumnTransformer(transformers=[('as_is',\n",
       "                                                  FunctionTransformer(func=<function <lambda> at 0x7fa44d7301f0>),\n",
       "                                                  ['weight', 'size', 'height',\n",
       "                                                   'age', 'rating']),\n",
       "                                                 ('ohe',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['rented for', 'body type',\n",
       "                                                   'category']),\n",
       "                                                 ('vec',\n",
       "                                                  ColumnTransformer(remainder='passthrough',\n",
       "                                                                    transformers=[('countvectorizer',\n",
       "                                                                                   CountVector...\n",
       "                                                                                                               'we',\n",
       "                                                                                                               'our',\n",
       "                                                                                                               'ours',\n",
       "                                                                                                               'ourselves',\n",
       "                                                                                                               'you',\n",
       "                                                                                                               \"you're\",\n",
       "                                                                                                               \"you've\",\n",
       "                                                                                                               \"you'll\",\n",
       "                                                                                                               \"you'd\",\n",
       "                                                                                                               'your',\n",
       "                                                                                                               'yours',\n",
       "                                                                                                               'yourself',\n",
       "                                                                                                               'yourselves',\n",
       "                                                                                                               'he',\n",
       "                                                                                                               'him',\n",
       "                                                                                                               'his',\n",
       "                                                                                                               'himself',\n",
       "                                                                                                               'she',\n",
       "                                                                                                               \"she's\",\n",
       "                                                                                                               'her',\n",
       "                                                                                                               'hers',\n",
       "                                                                                                               'herself',\n",
       "                                                                                                               'it',\n",
       "                                                                                                               \"it's\",\n",
       "                                                                                                               'its',\n",
       "                                                                                                               'itself', ...],\n",
       "                                                                                                   tokenizer=<function word_tokenize at 0x7fa4797b6820>),\n",
       "                                                                                   'review_summary')]),\n",
       "                                                  ['review_summary'])])),\n",
       "                ('randforest', RandomForestClassifier(max_depth=8))])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_cup3.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48246063462786487"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_cup3.score(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_bust = RandomForestClassifier(max_depth = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = train[['weight', 'size', 'height', 'age', 'rating', 'rented for', 'body type', 'category', 'review_summary']], train['bust_cat']\n",
    "test_X, test_Y = test[['weight', 'size', 'height', 'age', 'rating', 'rented for', 'body type', 'category', 'review_summary']], test['bust_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl4 = Pipeline(steps = [('transform', ct), ('random_forest', forest_bust)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianhuang/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('transform',\n",
       "                 ColumnTransformer(transformers=[('as_is',\n",
       "                                                  FunctionTransformer(func=<function <lambda> at 0x7fa44d7301f0>),\n",
       "                                                  ['weight', 'size', 'height',\n",
       "                                                   'age', 'rating']),\n",
       "                                                 ('ohe',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['rented for', 'body type',\n",
       "                                                   'category']),\n",
       "                                                 ('vec',\n",
       "                                                  ColumnTransformer(remainder='passthrough',\n",
       "                                                                    transformers=[('countvectorizer',\n",
       "                                                                                   CountVector...\n",
       "                                                                                                               'our',\n",
       "                                                                                                               'ours',\n",
       "                                                                                                               'ourselves',\n",
       "                                                                                                               'you',\n",
       "                                                                                                               \"you're\",\n",
       "                                                                                                               \"you've\",\n",
       "                                                                                                               \"you'll\",\n",
       "                                                                                                               \"you'd\",\n",
       "                                                                                                               'your',\n",
       "                                                                                                               'yours',\n",
       "                                                                                                               'yourself',\n",
       "                                                                                                               'yourselves',\n",
       "                                                                                                               'he',\n",
       "                                                                                                               'him',\n",
       "                                                                                                               'his',\n",
       "                                                                                                               'himself',\n",
       "                                                                                                               'she',\n",
       "                                                                                                               \"she's\",\n",
       "                                                                                                               'her',\n",
       "                                                                                                               'hers',\n",
       "                                                                                                               'herself',\n",
       "                                                                                                               'it',\n",
       "                                                                                                               \"it's\",\n",
       "                                                                                                               'its',\n",
       "                                                                                                               'itself', ...],\n",
       "                                                                                                   tokenizer=<function word_tokenize at 0x7fa4797b6820>),\n",
       "                                                                                   'review_summary')]),\n",
       "                                                  ['review_summary'])])),\n",
       "                ('random_forest', RandomForestClassifier(max_depth=8))])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl4.fit(train_X, train_Y_cup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3752775216039895"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl4.score(test_X, test_Y_cup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression for bust size\n",
    "#randomforestclassifier for cup size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
