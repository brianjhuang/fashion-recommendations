{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import regex as re\n",
    "from dateutil.parser import parse\n",
    "from fashionEDA import fashionCleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping Data, Null Entry: 1\n",
      "Scrapping Data, Null Entry: 2\n",
      "Scrapping Data, Null Entry: 3\n",
      "Scrapping Data, Null Entry: 4\n",
      "Scrapping Data, Null Entry: 5\n",
      "Scrapping Data, Null Entry: 6\n",
      "Scrapping Data, Null Entry: 7\n",
      "Scrapping Data, Null Entry: 8\n",
      "Scrapping Data, Null Entry: 9\n",
      "Scrapping Data, Null Entry: 10\n",
      "Scrapping Data, Null Entry: 11\n",
      "Scrapping Data, Null Entry: 12\n",
      "Scrapping Data, Null Entry: 13\n",
      "Scrapping Data, Null Entry: 14\n",
      "Scrapping Data, Null Entry: 15\n",
      "Scrapping Data, Null Entry: 16\n",
      "Scrapping Data, Null Entry: 17\n",
      "Scrapping Data, Null Entry: 18\n",
      "Scrapping Data, Null Entry: 19\n",
      "Scrapping Data, Null Entry: 20\n",
      "Scrapping Data, Null Entry: 21\n",
      "Scrapping Data, Null Entry: 22\n",
      "Scrapping Data, Null Entry: 23\n",
      "Scrapping Data, Null Entry: 24\n",
      "Scrapping Data, Null Entry: 25\n",
      "Scrapping Data, Null Entry: 26\n",
      "Scrapping Data, Null Entry: 27\n",
      "Scrapping Data, Null Entry: 28\n",
      "Scrapping Data, Null Entry: 29\n",
      "Scrapping Data, Null Entry: 30\n",
      "Scrapping Data, Null Entry: 31\n",
      "Scrapping Data, Null Entry: 32\n",
      "Scrapping Data, Null Entry: 33\n",
      "Scrapping Data, Null Entry: 34\n",
      "Scrapping Data, Null Entry: 35\n",
      "Scrapping Data, Null Entry: 36\n",
      "Scrapping Data, Null Entry: 37\n",
      "Scrapping Data, Null Entry: 38\n",
      "Scrapping Data, Null Entry: 39\n",
      "Scrapping Data, Null Entry: 40\n",
      "Scrapping Data, Null Entry: 41\n",
      "Scrapping Data, Null Entry: 42\n",
      "Scrapping Data, Null Entry: 43\n",
      "Scrapping Data, Null Entry: 44\n",
      "Scrapping Data, Null Entry: 45\n",
      "Scrapping Data, Null Entry: 46\n",
      "Scrapping Data, Null Entry: 47\n",
      "Scrapping Data, Null Entry: 48\n",
      "Scrapping Data, Null Entry: 49\n",
      "Scrapping Data, Null Entry: 50\n",
      "Scrapping Data, Null Entry: 51\n",
      "Scrapping Data, Null Entry: 52\n",
      "Scrapping Data, Null Entry: 53\n",
      "Scrapping Data, Null Entry: 54\n",
      "Scrapping Data, Null Entry: 55\n",
      "Scrapping Data, Null Entry: 56\n",
      "Scrapping Data, Null Entry: 57\n",
      "Scrapping Data, Null Entry: 58\n",
      "Scrapping Data, Null Entry: 59\n",
      "Scrapping Data, Null Entry: 60\n",
      "Scrapping Data, Null Entry: 61\n",
      "Scrapping Data, Null Entry: 62\n",
      "Scrapping Data, Null Entry: 63\n",
      "Scrapping Data, Null Entry: 64\n",
      "Scrapping Data, Null Entry: 65\n",
      "Scrapping Data, Null Entry: 66\n",
      "Scrapping Data, Null Entry: 67\n",
      "Scrapping Data, Null Entry: 68\n",
      "Scrapping Data, Null Entry: 69\n",
      "Scrapping Data, Null Entry: 70\n",
      "Scrapping Data, Null Entry: 71\n",
      "Scrapping Data, Null Entry: 72\n",
      "Scrapping Data, Null Entry: 73\n",
      "Scrapping Data, Null Entry: 74\n",
      "Scrapping Data, Null Entry: 75\n",
      "Scrapping Data, Null Entry: 76\n",
      "Scrapping Data, Null Entry: 77\n",
      "Scrapping Data, Null Entry: 78\n",
      "Scrapping Data, Null Entry: 79\n",
      "Scrapping Data, Null Entry: 80\n",
      "Scrapping Data, Null Entry: 81\n",
      "Scrapping Data, Null Entry: 82\n"
     ]
    }
   ],
   "source": [
    "fc = fashionCleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fc.getData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer/ Count Vectorizer Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brianhuang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/brianhuang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the NLTK English tokenizer and the stopwords of all languages\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added argument to this to include ngrams \n",
    "vectorizer = CountVectorizer(analyzer='word',ngram_range=(2, 2),\n",
    "                        max_features = 10000,\n",
    "                        tokenizer=word_tokenize,\n",
    "                        stop_words=stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True,\n",
    "                        analyzer='word',\n",
    "                        max_features=1500,\n",
    "                        tokenizer=word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size = 0.2, random_state = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bust Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = LogisticRegression()\n",
    "forest_mod = RandomForestClassifier(max_depth = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_is = ['weight', 'size', 'height', 'age', 'rating']\n",
    "ohe = ['rented for', 'body type', 'category']\n",
    "text_vec = ['review_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transformer = make_column_transformer((tfidf, 'review_summary'), remainder = 'passthrough')\n",
    "#replace vectorizer with either tfidf or vectorizer\n",
    "#column we want to vectorize/transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(\n",
    "    transformers = [(\"as_is\", FunctionTransformer(lambda x: x), as_is),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown = 'ignore'), ohe),\n",
    "    (\"vec\", text_transformer, text_vec)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_forest = Pipeline(steps = [('transform', ct), ('forest', forest_mod)])\n",
    "pl = Pipeline(steps = [('transform', ct), ('log_regression', mod)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = train[['weight', 'size', 'height', 'age', 'rating', 'rented for', 'body type', 'category', 'review_summary']], train['bust_cat']\n",
    "test_X, test_Y = test[['weight', 'size', 'height', 'age', 'rating', 'rented for', 'body type', 'category', 'review_summary']], test['bust_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianhuang/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('transform',\n",
       "                 ColumnTransformer(transformers=[('as_is',\n",
       "                                                  FunctionTransformer(func=<function <lambda> at 0x7f9092d264c0>),\n",
       "                                                  ['weight', 'size', 'height',\n",
       "                                                   'age', 'rating']),\n",
       "                                                 ('ohe',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['rented for', 'body type',\n",
       "                                                   'category']),\n",
       "                                                 ('vec',\n",
       "                                                  ColumnTransformer(remainder='passthrough',\n",
       "                                                                    transformers=[('tfidfvectorizer',\n",
       "                                                                                   TfidfVectorizer(max_features=1500,\n",
       "                                                                                                   sublinear_tf=True,\n",
       "                                                                                                   tokenizer=<function word_tokenize at 0x7f9092209820>),\n",
       "                                                                                   'review_summary')]),\n",
       "                                                  ['review_summary'])])),\n",
       "                ('forest', RandomForestClassifier(max_depth=8))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.fit(train_X, train_Y)\n",
    "pl_forest.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pl.predict(test_X)\n",
    "forest_preds = pl.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.591249103391741, 0.4825631041431841)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.score(test_X, test_Y), pl_forest.score(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cup Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training/test for X/Y but for cup categories\n",
    "train_X, train_Y_cup = train[['weight', 'size', 'height', 'age', 'rating', 'rented for', 'body type', 'category', 'review_summary']], train['cup_cat']\n",
    "test_X, test_Y_cup = test[['weight', 'size', 'height', 'age', 'rating', 'rented for', 'body type', 'category', 'review_summary']], test['cup_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cup_mod = RandomForestClassifier(max_depth = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_is = ['weight', 'size', 'height', 'age', 'rating']\n",
    "ohe = ['rented for', 'body type', 'category']\n",
    "text_vec = ['review_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cup_pl = Pipeline(steps = [('transform', ct), ('forest', cup_mod)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('transform',\n",
       "                 ColumnTransformer(transformers=[('as_is',\n",
       "                                                  FunctionTransformer(func=<function <lambda> at 0x7f9092d264c0>),\n",
       "                                                  ['weight', 'size', 'height',\n",
       "                                                   'age', 'rating']),\n",
       "                                                 ('ohe',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['rented for', 'body type',\n",
       "                                                   'category']),\n",
       "                                                 ('vec',\n",
       "                                                  ColumnTransformer(remainder='passthrough',\n",
       "                                                                    transformers=[('tfidfvectorizer',\n",
       "                                                                                   TfidfVectorizer(max_features=1500,\n",
       "                                                                                                   sublinear_tf=True,\n",
       "                                                                                                   tokenizer=<function word_tokenize at 0x7f9092209820>),\n",
       "                                                                                   'review_summary')]),\n",
       "                                                  ['review_summary'])])),\n",
       "                ('forest', RandomForestClassifier(max_depth=8))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cup_pl.fit(train_X, train_Y_cup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cup_preds = cup_pl.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3914335485193155"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cup_pl.score(test_X, test_Y_cup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictionConverter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-8a8eaf46128a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictionConverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcup_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbust\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predictionConverter' is not defined"
     ]
    }
   ],
   "source": [
    "pc = predictionConverter(cup = cup_preds, bust = preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
